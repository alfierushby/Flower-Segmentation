@inproceedings{chenEncoderDecoderAtrousSeparable2018,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  pages = {833--851},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01234-2_49},
  abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.},
  isbn = {978-3-030-01234-2},
  langid = {english},
  keywords = {Depthwise separable convolution,Encoder-decoder,Semantic image segmentation,Spatial pyramid pooling},
  file = {C:\Users\Winothy\Zotero\storage\9NAPVN32\Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf}
}

@InProceedings{resnet18,
  title = 	 {ResNet-18},
  url = 	 {https://uk.mathworks.com/help/deeplearning/ref/resnet18.html},
  abstract = 	 {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder   variants with impressive results being obtained in several areas, mostly   on vision and language datasets.  The best results obtained on supervised   learning tasks often involve an unsupervised learning component, usually   in an unsupervised pre-training phase. The main question investigated   here is the following: why does unsupervised pre-training work so well?   Through extensive experimentation, we explore several possible   explanations discussed in the literature including its action as a   regularizer (Erhan et al. 2009) and as an aid to optimization   (Bengio et al. 2007).  Our results build on the work of   Erhan et al. 2009, showing that unsupervised pre-training appears to   play predominantly a regularization role in subsequent supervised   training. However our results in an online setting, with a virtually unlimited   data stream, point to a somewhat more nuanced interpretation of the roles   of optimization and regularization in the unsupervised pre-training   effect.}
}


@InProceedings{pmlr-v9-erhan10a,
  title = 	 {Why Does Unsupervised Pre-training Help Deep Learning?},
  author = 	 {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {201--208},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/erhan10a/erhan10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/erhan10a.html},
  abstract = 	 {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder   variants with impressive results being obtained in several areas, mostly   on vision and language datasets.  The best results obtained on supervised   learning tasks often involve an unsupervised learning component, usually   in an unsupervised pre-training phase. The main question investigated   here is the following: why does unsupervised pre-training work so well?   Through extensive experimentation, we explore several possible   explanations discussed in the literature including its action as a   regularizer (Erhan et al. 2009) and as an aid to optimization   (Bengio et al. 2007).  Our results build on the work of   Erhan et al. 2009, showing that unsupervised pre-training appears to   play predominantly a regularization role in subsequent supervised   training. However our results in an online setting, with a virtually unlimited   data stream, point to a somewhat more nuanced interpretation of the roles   of optimization and regularization in the unsupervised pre-training   effect.}
}

@article{shortenSurveyImageData2019,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  date = {2019-12},
  journaltitle = {Journal of Big Data},
  shortjournal = {J Big Data},
  volume = {6},
  number = {1},
  pages = {1--48},
  publisher = {SpringerOpen},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
  urldate = {2024-05-05},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  issue = {1},
  langid = {english},
  file = {C:\Users\Winothy\Zotero\storage\SA8EA2BH\Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf}
}


@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2024-05-05},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {C:\Users\Winothy\Zotero\storage\M5K9ZXBD\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@article{badrinarayananSegNetDeepConvolutional2017,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Image Segmentation}}.},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  date = {2017-12},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0162-8828},
  url = {https://www.repository.cam.ac.uk/handle/1810/271007},
  urldate = {2024-05-05},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet.},
  langid = {english},
  file = {C:\Users\Winothy\Zotero\storage\MK66BLKI\Badrinarayanan et al. - 2017 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf}
}


@article{nilsbackDelvingDeeperWhorl2010,
  title = {Delving Deeper into the Whorl of Flower Segmentation},
  author = {Nilsback, Maria-Elena and Zisserman, Andrew},
  date = {2010-06-01},
  journaltitle = {Image Vision Comput.},
  shortjournal = {Image Vision Comput.},
  volume = {28},
  pages = {1049--1062},
  doi = {10.1016/j.imavis.2009.10.001},
  abstract = {We describe an algorithm for automatically segmenting flowers in colour photographs. This is a challeng- ing problem because of the sheer variety of flower classes, the variability within a class and within a par- ticular flower, and the variability of the imaging conditions - lighting, pose, foreshortening, etc. The method couples two models - a colour model for foreground and background, and a light generic shape model for the petal structure. This shape model is tolerant to viewpoint changes and petal defor- mations, and applicable across many different flower classes. The segmentations are produced using a MRF cost function optimized using graph cuts. We show how the components of the algorithm can be tuned to overcome common segmentation errors, and how performance can be optimized by learning parameters on a training set. The algorithm is evaluated on 13 flower classes and more than 750 examples. Performance is assessed against ground truth trimap segmentations. The algorithms is also compared to several previous approaches for flower segmentation.}
}


@inproceedings{nilsbackVisualVocabularyFlower2006,
  title = {A {{Visual Vocabulary}} for {{Flower Classification}}},
  author = {Nilsback, M.-E and Zisserman, A.},
  date = {2006-02-01},
  volume = {2},
  pages = {1447--1454},
  doi = {10.1109/CVPR.2006.42},
  abstract = {We investigate to what extent bag of visual words models can be used to distinguish categories which have significant visual similarity. To this end we develop and optimize a nearest neighbour classifier architecture, which is evaluated on a very challenging database of flower images. The flower categories are chosen to be indistinguishable on colour alone (for example), and have considerable variation in shape, scale, and viewpoint. We demonstrate that by developing a visual vocabulary that explicitly represents the various aspects (colour, shape, and texture) that distinguish one flower from another, we can overcome the ambiguities that exist between flower categories. The novelty lies in the vocabulary used for each aspect, and how these vocabularies are combined into a final classifier. The various stages of the classifier (vocabulary selection and combination) are each optimized on a validation set. Results are presented on a dataset of 1360 images consisting of 17 flower species. It is shown that excellent performance can be achieved, far surpassing standard baseline algorithms using (for example) colour cues alone.},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  isbn = {978-0-7695-2597-6}
}


@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  url = {https://ieeexplore.ieee.org/document/7780459},
  urldate = {2024-04-29},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {C\:\\Users\\Winothy\\Zotero\\storage\\UHJNL58U\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\Winothy\\Zotero\\storage\\JRS3TP5B\\7780459.html}
}

@online{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-03-02},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1502.03167},
  url = {http://arxiv.org/abs/1502.03167},
  urldate = {2024-04-30},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Winothy\\Zotero\\storage\\V62SCZDS\\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;C\:\\Users\\Winothy\\Zotero\\storage\\GB98Y3RC\\1502.html}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} – {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  date = {2015},
  pages = {234--241},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  file = {C:\Users\Winothy\Zotero\storage\PVMN9Z6U\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@article{sandfortDataAugmentationUsing2019,
  title = {Data Augmentation Using Generative Adversarial Networks ({{CycleGAN}}) to Improve Generalizability in {{CT}} Segmentation Tasks},
  author = {Sandfort, Veit and Yan, Ke and Pickhardt, Perry J. and Summers, Ronald M.},
  date = {2019-11-15},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {9},
  number = {1},
  pages = {16884},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-52737-x},
  url = {https://www.nature.com/articles/s41598-019-52737-x},
  urldate = {2024-04-30},
  abstract = {Labeled medical imaging data is scarce and expensive to generate. To achieve generalizable deep learning models large amounts of data are needed. Standard data augmentation is a method to increase generalizability and is routinely performed. Generative adversarial networks offer a novel method for data augmentation. We evaluate the use of CycleGAN for data augmentation in CT segmentation tasks. Using a large image database we trained a CycleGAN to transform contrast CT images into non-contrast images. We then used the trained CycleGAN to augment our training using these synthetic non-contrast images. We compared the segmentation performance of a U-Net trained on the original dataset compared to a U-Net trained on the combined dataset of original data and synthetic non-contrast images. We further evaluated the U-Net segmentation performance on two separate datasets: The original contrast CT dataset on which segmentations were created and a second dataset from a different hospital containing only non-contrast CTs. We refer to these 2 separate datasets as the in-distribution and out-of-distribution datasets, respectively. We show that in several CT segmentation tasks performance is improved significantly, especially in out-of-distribution (noncontrast CT) data. For example, when training the model with standard augmentation techniques, performance of segmentation of the kidneys on out-of-distribution non-contrast images was dramatically lower than for in-distribution data (Dice score of 0.09 vs. 0.94 for out-of-distribution vs. in-distribution data, respectively, p\,{$<$}\,0.001). When the kidney model was trained with CycleGAN augmentation techniques, the out-of-distribution (non-contrast) performance increased dramatically (from a Dice score of 0.09 to 0.66, p\,{$<$}\,0.001). Improvements for the liver and spleen were smaller, from 0.86 to 0.89 and 0.65 to 0.69, respectively. We believe this method will be valuable to medical imaging researchers to reduce manual segmentation effort and cost in CT imaging.},
  langid = {english},
  keywords = {Diagnostic markers,Image processing},
  file = {C:\Users\Winothy\Zotero\storage\2YS6V4FN\Sandfort et al. - 2019 - Data augmentation using generative adversarial net.pdf}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  urldate = {2024-04-30},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {C:\Users\Winothy\Zotero\storage\IACH3IRR\Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@article{szegedyInceptionv4InceptionResNetImpact2016,
  title = {Inception-v4, {{Inception-ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  date = {2016-02-23},
  journaltitle = {AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI Conference on Artificial Intelligence},
  volume = {31},
  doi = {10.1609/aaai.v31i1.11231},
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
  file = {C:\Users\Winothy\Zotero\storage\3XSHM8UR\Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of R.pdf}
}

@inproceedings{thanapolReducingOverfittingImproving2020,
  title = {Reducing {{Overfitting}} and {{Improving Generalization}} in {{Training Convolutional Neural Network}} ({{CNN}}) under {{Limited Sample Sizes}} in {{Image Recognition}}},
  booktitle = {2020 - 5th {{International Conference}} on {{Information Technology}} ({{InCIT}})},
  author = {Thanapol, Panissara and Lavangnananda, Kittichai and Bouvry, Pascal and Pinel, Frédéric and Leprévost, Franck},
  date = {2020-10},
  pages = {300--305},
  doi = {10.1109/InCIT50588.2020.9310787},
  url = {https://ieeexplore.ieee.org/document/9310787},
  urldate = {2024-04-29},
  abstract = {In deep learning, application of Convolutional Neural Network (CNN) is prolific in image recognition. CNN assumes that large amount of samples are available in the dataset in order to implement an effective CNN model. However, this assumption may not be practical or possible in some real world applications. It is commonly known that training a CNN model under limited samples available often leads to overfitting and inability to generalize. Data augmentation, batch normalization and dropout techniques have been suggested to mitigate such problems. This work studies the effect of overfitting and generalization in image recognition of intentionally contracted CIF AR-10 dataset. Application of these techniques and their combination are considered as well as injection of data augmentation at different epochs. The result of this work reveals that utilizing injection at 30 epoch in the application of width and height shift data augmentation together with dropout yields the best performance and can overcome the overfitting effect best.},
  eventtitle = {2020 - 5th {{International Conference}} on {{Information Technology}} ({{InCIT}})},
  keywords = {CIFAR-10 Dataset,Convolution Neural Network (CNN),Data models,Deep learning,Feature extraction,Generalization,Image color analysis,Image recognition,Image Recognition,Neural networks,Overfitting,Training},
  file = {C:\Users\Winothy\Zotero\storage\SHQK75X4\9310787.html}
}

@inproceedings{zophLearningTransferableArchitectures2018,
  title = {Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}},
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc},
  date = {2018-06-01},
  pages = {8697--8710},
  doi = {10.1109/CVPR.2018.00907},
  file = {C:\Users\Winothy\Zotero\storage\H7L74GAL\Zoph et al. - 2018 - Learning Transferable Architectures for Scalable I.pdf}
}
